<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
  <head>
    <title>How to perform analyses in the light of ASA’s recent statement on p-values</title>
    <meta charset="utf-8" />
    <meta name="author" content="Matthew Loop" />
    <meta name="date" content="2019-08-15" />
    <link rel="stylesheet" href="xaringan-themer.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# How to perform analyses in the light of ASA’s recent statement on p-values
### Matthew Loop
### UNC Chapel Hill
### 2019-08-15

---






# An inconvenient statement

The American Statistical Association (ASA) released a statement in March urging the scientific community to abandon the use of the phrase "statistically significant." Specifically, it said, 

&gt; We conclude, based on our review of the articles in this special issue and the broader literature, that it is time to stop using the term “statistically significant” entirely. Nor should variants such as “significantly different,” “p &lt; 0.05,” and “nonsignificant” survive, whether expressed in words, by asterisks in a table, or in some other way.
&gt; 
&gt; &lt;footer&gt;--- Ron Wasserstein, Allen Schirm, and Nicole Lazar&lt;/footer&gt;

---

# "Seriously?!" said all the scientists

.center[
![](https://media.giphy.com/media/l2SpLQgNpVORsiqVW/giphy.gif)
]

---

class: inverse, center, middle

# Let's give some context...

---
# Previous ASA recommendations in 2016

&gt; * Don’t base your conclusions solely on whether an association or effect was found to be “statistically significant” (i.e., the p-value passed some arbitrary threshold such as `\(p &lt; 0.05\)`).  
&gt; * Don’t believe that an association or effect exists just because it was statistically significant.  
&gt; * Don’t believe that an association or effect is absent just because it was not statistically significant.  
&gt; * Don’t believe that your p-value gives the probability that chance alone produced the observed association or effect or the probability that your test hypothesis is true.  
&gt; * Don’t conclude anything about scientific or practical importance based on statistical significance (or lack thereof)
&gt; 
&gt; &lt;footer&gt;--- Ron Wasserstein, Allen Schirm, and Nicole Lazar&lt;/footer&gt;
---



# So statisticians are telling me I'm wrong...again! Jerks.

.left-column[
![](https://media.giphy.com/media/wVcNP3TnXbl84/giphy.gif)
]

.right-column[
&gt; Knowing what not to do with p-values is indeed necessary, but it does not suffice. It is as though statisticians were asking users of statistics to tear out the beams and struts holding up the edifice of modern scientific research without offering solid construction materials to replace them. Pointing out old, rotting timbers was a good start, but now we need more.
&gt; 
&gt; &lt;footer&gt;--- Ron Wasserstein, Allen Schirm, and Nicole Lazar&lt;/footer&gt;
]



---

# My background

.middle[
.pull-left[
* Biology (evolution/ecology), but PhD in Biostatistics and work as an applied statistician
* Was always interested in how data were actually used to evaluate theories
* Served as both lead author and collaborating statistician on applied papers
* I've said "statistically significant" about p-values quite a lot
]

.pull-right[
![](https://sph.unc.edu/files/2016/09/Matthew_Loop_Profile_2016.jpg)
]
]

---

# Difficulties of new recommendation

1. not sure what to write in Results and Discussion sections
2. probability of acceptance at a given journal may go down
3. more effort required for same payoff (i.e., publication)

---

# Benefits

1. protect you against false positives
2. protect you against false negatives
3. your statistics/results/discussion sections appropriately reflect your Scientific uncertainty

---
# Resources that have helped me

.pull-left[
&lt;img src="http://www.stat.columbia.edu/~gelman/arm/cover.gif" height="350px" /&gt;
]


.pull-right[
&lt;img src="https://xcelab.net/rm/wp-content/uploads/2012/01/9781482253443.jpg" height="350px" /&gt;
]

---

# Resources that have helped me

.pull-left[
![](http://www.stat.columbia.edu/~gelman/me_2016.jpg)

[Andrew Gelman's Blog](https://statmodeling.stat.columbia.edu)
]

.pull-right[
![](https://www.vumc.org/biostatistics/sites/vumc.org.biostatistics/files/people/FrankHarrell.jpg)

[Frank Harrell's Blog](https://www.fharrell.com/post/)
]

---

class: middle, center, inverse

# 3 specific recommendations

---

class: inverse, middle, center

## 1. Talk about risk/odds/hazard ratios that are consistent with the data

---

So if you aren't supposed to say something is "statistically significant," what do you say? I've started thinking about my results by asking the following questions:

1. what values are consistent with the data; and
2. what values are inconsistent/ruled out by the data?

For example let's say you estimated an odds ratio of 1.1 with a confidence interval of (0.95, 1.27) for the treatment versus control group on medication adherence over the next 7 days. Now, in the old paradigm, I would say that there was "no statistically significant evidence of an association between the treatment on medication adherence." This interpretation comes strictly from the null hypothesis significance testing framework, where we claim all we are interested in is whether the odds ratio is 1 or not. But in reality, it's unlikely any treatment has an effect of exactly 0. 
---

Nowadays, I look at a result like this and think the following:

&gt; Well, our best estimate is that the odds are 10% higher for being adherent in the treatment group compared to the control. In fact, the data are consistent with having a 25% increase in adherence, which would be great! However, because of the large amount of uncertainty, we can't rule out the fact that the treatment may reduce adherence. How can we reduce variation in our estimate? Are we sure the treatment was administered in exactly the same way? Do we have better, more accurrate ways to measure adherence? Is there a lot of measurement error? Are there other variables that do a good job of predicting adherence, so that we can reduce the uncertainty in the estimate of our treatment effect? If I were going to bet on it, the treatment is probably helpful, given that a larger proportion of the odds ratios consistent with the data are positive. But we need to rule out a negative effect, if this treatment is truly effective. Let's do another study...

---
class: middle
This reaction, and the follow up actions, are much different than saying, "Well, we didn't find an effect. So there probably isn't one."

.footnote[Even though that's the incorrect interpreation of a p value, that's often how they are used in practice.]

---

class: inverse, middle, center

## 2. Ask yourself, "Did I learn anything?"

---
class: middle
Now let's the previous scenario a bit, where your traditional interpretation would be the same as the previous one.

Let's assume that the odds ratio you estimated was 1.05 (0.1, 11.03). The traditional approach would be the same: "we found no statistically significant evidence of an effect of treatment on adherence."

Today, I would say something different. **I would put forth that you learned absolutely nothing from doing this study.**

---

My thinking goes like this:

&gt; I do observational research on behavioral interventions on adherence. Most odds ratios tend to be small, with a high probability of being between 0.8 and 1.2. An effect bigger than that would be considered absolutely huge and a big breakthrough. Now, I just got this range of values consistent with the data from 0.1 to 11.03. But I was expected any real treatment effect to be much closer to one. I haven't learned anything.

A result like this definitely brings to light major problems with the study. It's likely you didn't have enough sample size, or your measurements were too noisy, to really be able to study the effect of your intervention at all. A bigger study, with better measurements, is definitely needed to answer the question.

---
class: inverse, middle, center
## 3. Avoid treating statistics as "uncertainty laundering" 

.footnote.left.tiny[Term 'uncertainty laundering' comes from Andrew Gelman in this same editorial as well as at [](https://statmodeling.stat.columbia.edu/2016/03/07/29212/)]

---
class: center, middle

&gt; ... it seems to me that statistics is often sold as a sort of alchemy that transmutes randomness into certainty, an “uncertainty laundering” that begins with data and concludes with success as measured by statistical significance. Again, I do not exempt my own books from this criticism: we present neatly-packaged analyses with clear conclusions. This is what is expected --demanded-- of subject-matter journals. Just try publishing a result with p = 0.20. If researchers have been trained with the expectation that they will get statistical significance if they work hard and play by the rules, if granting agencies demand power analyses in which researchers must claim 80% certainty that they will attain statistical significance, and if that threshold is required for publication, it is no surprise that researchers will routinely satisfy this criterion, and publish, and publish, and publish, even in the absence of any real effects, or in the context of effects that are so variable as to be undetectable in the studies that are being conducted (Gelman and Carlin, 2014).
&gt; 
&gt; --- Andrew Gelman, 2016


---
# Uncertainty remains after analysis

* Somehow, the p value became the ultimate and final judge of whether we had "found something"
* We're simply more comfortable with binary thinking? (e.g., This study found something; this one didn't. This treatment improves cancer remission rates; this one doesn't. )
* We shouldn't let a tool that was designed to help make a *decision* about what to do next stand in for our hard thinking about what is a *fact.*
* Statistics: data --&gt; model + uncertainty

---
# Uncertainty remains after analysis

* We never have complete certainty. A treatment is never "definitely helpful" or "definitely harmful"
* Even when we do have significant results, it doesn't mean that the result was reliable or that there isn't a huge amount of uncertainty. 
* To go with a previous example, let's say the estimated odds ratio for the treatment group compared to the control on adherence was 3 with a confidence interval between 1.01 and 8.91. What should we conclude from this result. Based on our prior knowledge, this result is almost certainly an overestimate, *if we rely on the p-value to filter which results we will concentrate on.* The only reason we are considering this estimate "significant" is that the 95% confidence interval did not include 1.

---
class: inverse, middle, center

# An example

---

# Rewriting a results section

My Master's paper was titled, ["Fine particulate matter and incident cognitive impairment in the REasons for Geographic and Racial Differences in Stroke (REGARDS) cohort"](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0075001). Here is Table 5 from the paper:
![](figure.png)&lt;!-- --&gt;
---

And the text that accompanied it:

"Of the total sample (n=20,150), 1,633 participants (8%) were classified as cognitively impaired at their most recent follow-up. Table 5 shows the odds ratios and approximate 95% Wald’s confidence intervals for a 10 µg/m3 change in PM2.5 for each of the four models, both for the main analysis (impaired on the most recent assessment) and sensitivity analysis (impaired on both of the two most recent assessments). Model 1 indicated that there was no association between PM2.5 and the odds of incident cognitive impairment. The odds ratio decreased sharply after demographics were added (model 2), with an odds ratio close to 1. This estimated odds ratio was more or less constant for models 3 and 4."

---

So my writing obviously violated the ASA's current feelings on p values. Here is what I would do if I were writing this up today:

"Model 1 suggested that a 10 µg/m3 higher annual average exposure to PM2.5 was associated with an approximately 25% higher odds of cognitive impairment. However, this interval included implausiblly high odds ratios, as well as some odds ratios that would indicate an association with lower odds of cognitive impairment among those with higher exposure to PM2.5. After adjustment for additional potential confounders, the odds of cognitive impairment were very similar between someone with a, say, 15 µg/m3 vs. a 5 µg/m3 annual average exposure to PM2.5. The confidence intervals also indicated the data was consistent with large negative and positive associations between PM2.5 exposure and cognitive impairment. Therefore, it is difficult to make any confident statements about the relationship, based upon our data."

---

If I had been interpreting the results this way, I may have 

* looked deeper into the variability in the measurement of air pollution we were using, and potentially try to model it in order to create a better model
* investigated more who we were doing a poor job of predicting their risk of having cognitive impairment 

Instead, I made the classic mistake of equating "nonsignificant" with "nonexistent", and in the first model "significant" with "real."

.footnote[I also recognize that it's easy for me to give a critical review of the data like this now that it's published. I already have credit for it and am not worried about it being rejected because I'm taking such a cautious approach. I'm trying to do better.]

---

# Responding to a journal reviewer

I haven't encountered a belligerent reviewer yet on this issue, but I'm sure I will. Let's say you follow the advice above and avoid using the phrase "statistically significant," and you get the following review:

&gt; The authors have not performed hypothesis tests for their claims, and so it is unclear what rigor their results have. Using p values is so important that it's how we approve medical treatments. Therefore, it's difficult to ascertain the strength of the authors' findings without them assigning a predetermined type 1 error rate to their p values and then testing the null hypothesis of no effect. How do we know to trust the effect or not? Is the signal greater than the noise? 

---
Here's what my response might look like

&gt; We thank the reviewer for their prudent comment. There has been much discussion recently about the use of p-values to test scientific hypotheses, and much of the discussion concentrates on the damage the p value has caused to the reproducibility of scientific studies.[1] In order to protect ourselves against overly optimistic conclusions, we have decided to clearly represent: (a) the best estimate for the mean difference between treatment and control; and (b) the uncertainty we have about the mean difference, using a confidence interval. Our uncetainty interval indicated that our data is consistent with many positive values of the treatment effect, but we could not rule out small, harmful effects of the treatment. These harmful effects that were consistent with the data are not clinically meaningful. Therefore, we have concluded that the treatment likely has a helpful effect.

.footnote[
[1] Wasserstein RL, Schirm AL, Lazar NA. Moving to a World Beyond “p &lt; 0.05.” Am Stat. Taylor &amp; Francis; 2019;73:1–19.; 
]

---

# Benefits of abandoning "statistical significance"

1. you get to think about the veracity of estimates of effects, without being beholden to an arbitrary cutoff
2. you carry forward a better mental map of your uncertainty about the question --&gt; claims are more reproducible
3. estimate + uncertainty helps you think more about whether your estimate is believable

---

![](nejm.png)&lt;!-- --&gt;

A small step, but now does not allow p-values for unplanned comparisons with no prespecified method of type 1 error correction
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();</script>

<script>
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>

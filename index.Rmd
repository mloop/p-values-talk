---
title: "How to perform analyses in the light of ASA’s recent statement on p-values"
author: "Matthew Loop"
date: "`r Sys.Date()`"
output: tint::tintHtml
bibliography: skeleton.bib
link-citations: yes
---

```{r setup, include=FALSE}
library(tint)
# invalidate cache when the package version changes
knitr::opts_chunk$set(tidy = FALSE, cache.extra = packageVersion('tint'))
options(htmltools.dir.version = FALSE)
```

# Introduction

The American Statistical Association (ASA) released a statement in March urging the scientific community to abandon the use of the phrase "statistically significant." Specifically, it said, 

> We conclude, based on our review of the articles in this special issue and the broader literature, that it is time to stop using the term “statistically significant” entirely. Nor should variants such as “significantly different,” “p < 0.05,” and “nonsignificant” survive, whether expressed in words, by asterisks in a table, or in some other way.
> 
> `r tint::quote_footer('--- Ron Wasserstein, Allen Schirm, and Nicole Lazar')`

Why did the ASA make this recommendation? Well, earlier in the editorial they mention the following:

> If you’re just arriving to the debate, here’s a sampling of what not to do:

> •  Don’t base your conclusions solely on whether an association or effect was found to be “statistically significant” (i.e., the p-value passed some arbitrary threshold such as $p < 0.05$).  
> •  Don’t believe that an association or effect exists just because it was statistically significant.  
> •  Don’t believe that an association or effect is absent just because it was not statistically significant.  
> •  Don’t believe that your p-value gives the probability that chance alone produced the observed association or effect or the probability that your test hypothesis is true.  
> •  Don’t conclude anything about scientific or practical importance based on statistical significance (or lack thereof)

# The problem

So statisticians are telling me I'm wrong...again! What am I supposed to do?

> Don’t. Don’t. Just...don’t. Yes, we talk a lot about don’ts. The ASA Statement on p-Values and Statistical Significance (Wasserstein and Lazar 2016) was developed primarily because after decades, warnings about the don’ts had gone mostly unheeded. The statement was about what not to do, because there is widespread agreement about the don’ts. Knowing what not to do with p-values is indeed necessary, but it does not suffice. It is as though statisticians were asking users of statistics to tear out the beams and struts holding up the edifice of modern scientific research without offering solid construction materials to replace them. Pointing out old, rotting timbers was a good start, but now we need more.


# 3 recommendations for what to do
`r margin_note("Most of my opinions about this and how I've changed in my interpretations of p-values have to do with the thoughts of [Frank Harrell](https://www.fharrell.com/post/) and [Andrew Gelman](https://statmodeling.stat.columbia.edu). I've borrowed heavily from them, and I highly recommend that you do the same.")`

## 1. Talk about risk/odds/hazard ratios that are consistent with the data


## 2. Ask yourself, "Did I learn anything?"



## 3. Avoid treating statistics as "uncertainty laundering" 

`r margin_note("Term 'uncertainty laundering' comes from Andrew Gelman in this same editorial as well as at https://statmodeling.stat.columbia.edu/2016/03/07/29212/")`

# How to do it

## Rewriting a results section
```{r, fig.margin = TRUE, echo=FALSE}
knitr::include_graphics("figure.png")
```

*Of the total sample ($n=20,150$), 1,633 participants (8%) were classified as cognitively impaired at their most recent follow-up. Table 5 shows the odds ratios and approximate 95% Wald’s confidence intervals for a 10 µg/m3 change in PM$_{2.5}$ for each of the four models, both for the main analysis (impaired on the most recent assessment) and sensitivity analysis (impaired on both of the two most recent assessments). Model 1 indicated that there was no association between PM2.5 and the odds of incident cognitive impairment. The odds ratio decreased sharply after demographics were added (model 2), with an odds ratio close to 1. This estimated odds ratio was more or less constant for models 3 and 4.*


## Responding to a journal reviewer

# How the world could be different